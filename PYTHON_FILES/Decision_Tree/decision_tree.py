# -*- coding: utf-8 -*-
"""Decision_Tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q9JD2c-WMln6884-1Cnbhct5_UciBXhj
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Python Training of Decision Tree, and obtaining parameters"""

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.compose import ColumnTransformer
import joblib  # For saving the model
import graphviz  # For visualizing the tree

# Load the dataset
file_path = '/content/drive/MyDrive/Final_Dataset.xlsx'  # File path for the dataset
df = pd.read_excel(file_path)

# Feature and target separation
X = df.drop(columns=['Output'])  # Assuming 'Output' is the target column
y = df['Output']

# Identify categorical columns for one-hot encoding
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
#print(f"Categorical features: {categorical_features}")


# Column transformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(), categorical_features)],
    remainder='passthrough'  # Keeps other columns as they are
)

# Apply preprocessing
X = preprocessor.fit_transform(X)

total_features = preprocessor.get_feature_names_out()  # Get feature names after preprocessing
# print(f"Total features after preprocessing: {len(total_features)}")
#print(f"Feature names: {total_features}")

# Convert target to categorical format (if necessary)
y = y - 1  # Adjust classes to start from 0 for sklearn if classes are not starting from 0

# Split dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# these 50 samples are for hls, python verification
# Assume `X_test` and `y_test` are already prepared after splitting
# Extract 50 samples from the test set
num_samples = 50
X_test_samples = X_val[:num_samples] # First 50 samples from test features
y_test_samples = y_val[:num_samples]  # First 50 samples from test labels
#print(X_test_samples)
# print(y_test_samples)
# Convert to numpy arrays if necessary
X_test_samples = np.array(X_test_samples)
y_test_samples = np.array(y_test_samples)
# Save the test samples to a CSV file
np.savetxt('X_test_samples.csv', X_test_samples, delimiter=',')
np.savetxt('y_test_samples.csv', y_test_samples, delimiter=',', fmt='%d')  # Save labels as integers

print("Test samples saved!")

# Initialize and train the Decision Tree model
decision_tree = DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=5, max_depth=8,random_state=42)
#decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)

# Save the trained model
joblib.dump(decision_tree, 'decision_tree_model.pkl')

# Load test inputs
#test_inputs = np.loadtxt('test_inputs.csv', delimiter=',')

# Get predictions from the Python model
python_predictions = decision_tree.predict(X_test_samples)


# Save predictions for comparison
np.savetxt('python_predictions_compare_with_y_test_samples.csv', python_predictions, fmt='%d', delimiter=',')

# Evaluate the model on validation data
y_pred = decision_tree.predict(X_val)
accuracy = np.mean(y_pred == y_val)

print(f"Validation Accuracy: {accuracy:.2f}")

# Generate a classification report
print("\nClassification Report:")
print(classification_report(y_val, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_val, y_pred))

# Visualize the decision tree
dot_data = export_graphviz(
    decision_tree,
    out_file=None,
    feature_names=preprocessor.get_feature_names_out(),
    class_names=np.unique(y).astype(str),
    filled=True,
    rounded=True,
    special_characters=True
)
graph = graphviz.Source(dot_data)
graph.render("decision_tree")  # Saves visualization as a file
graph.view()  # Opens the visualization

"""## To Create HLS Testbench, this will generate HLS testbench based on the parameters obtained from training:"""

#For creating HLS testbench

import csv

# File paths

def load_x_test_samples(file):
    x_test_samples = []
    with open(file, 'r') as f:
        reader = csv.reader(f, delimiter=',')  # Changed delimiter to ','
        for row in reader:
            x_test_samples.append([int(float(val)) for val in row]) # Convert to float first, then int
    return x_test_samples

# Load y_test_samples
def load_y_test_samples(file):
    y_test_samples = []
    with open(file, 'r') as f:
        reader = csv.reader(f)
        for row in reader:
            y_test_samples.append(int(row[0]))
    return y_test_samples

# Main function
def main():
    # Load inputs and outputs
    x_test_samples = load_x_test_samples("/content/X_test_samples.csv")
    y_test_samples = load_y_test_samples("/content/y_test_samples.csv")

    # Generate C++ testbench code
    testbench_code = """
#include "decision_tree.h"
#include <iostream>

int main() {
    ap_children children_left[NUM_NODES] = { /* Fill with values */ };
    ap_children children_right[NUM_NODES] = { /* Fill with values */ };
    ap_feature_split feature_split[NUM_NODES] = { /* Fill with values */ };
    ap_threshold threshold[NUM_NODES] = { /* Fill with values */ };
    ap_value value[NUM_NODES] = { /* Fill with values */ };
    ap_parent parent[NUM_NODES] = { /* Fill with values */ };
    int matches = 1; // Assume all inputs will match
    ap_input_features x; // Declare x once

      // Test inputs and expected outputs
      for (int i = 0; i < 50; i++) {
        // Assign the feature values based on the input test samples
        switch (i) {
    """

    for i, (x_sample, y_sample) in enumerate(zip(x_test_samples, y_test_samples)):
        feature_vector = ", ".join(map(str, x_sample))
        testbench_code += f"""
          case {i}: x = {{ {feature_vector} }}; break;"""

    testbench_code += """
        }
        ap_value result = decision_tree(x, children_left, children_right, feature_split, threshold, value, parent);
        int expected = 1; // Set the expected result for each case
        switch (i) {
      """

    for i, (x_sample, y_sample) in enumerate(zip(x_test_samples, y_test_samples)):
        feature_vector = str(y_sample)
        testbench_code += f"""
          case {i}: expected = {{ {feature_vector} }}; break;"""

    testbench_code += """
        }

        if (result != expected) {
            std::cout << "Mismatch for input " << i + 1 << ": Expected " << expected << ", Got " << (int)result << std::endl;
            matches = 0;
        }
    }

    if (matches) {
        std::cout << "All inputs matched the expected outputs." << std::endl;
    } else {
        std::cout << "Some inputs did not match the expected outputs." << std::endl;
    }

    return 0;
}
    """

    # Save the generated testbench code
    with open("testbench.cpp", "w") as f:
        f.write(testbench_code)
    print("Testbench generated and saved to testbench.cpp")

if __name__ == "__main__":
    main()

"""## Testing for sample data"""

#This input can be taken from excel file, and then send the output to htmp, note 0, 1, 2, 3, 4 output class will be mapped to 1, 2, 3, 4, 5

input_test_python = [[0,1,0,0,0,30,88,-12],
                     [0,0,0,1,0,28,3,53],
                     [0,0,1,0,0,26,5,45],
                     [0,0,0,1,0,21,76,58],
                     [0,0,1,0,0,51,4,-18]]

predictions_python = decision_tree.predict(input_test_python)
print("Python Inference of decision tree:\n Input: ",input_test_python)
print("Corresponding Output: ",predictions_python)